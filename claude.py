# =============================================================================
# QWEN3-0.6B FINE-TUNING TRÃŠN GOOGLE COLAB
# HÆ°á»›ng dáº«n chi tiáº¿t tá»« A-Z cho ngÆ°á»i má»›i báº¯t Ä‘áº§u
# =============================================================================

# =============================================================================
# BÆ¯á»šC 1: KIá»‚M TRA VÃ€ CÃ€I Äáº¶T MÃ”I TRÆ¯á»œNG
# =============================================================================

print("ğŸ” KIá»‚M TRA MÃ”I TRÆ¯á»œNG GOOGLE COLAB")
print("=" * 50)

# Kiá»ƒm tra GPU
!nvidia-smi

# Kiá»ƒm tra Python version
import sys
print(f"ğŸ Python version: {sys.version}")

# Kiá»ƒm tra packages hiá»‡n táº¡i
import pkg_resources
installed_packages = [d.project_name for d in pkg_resources.working_set]
key_packages = ['torch', 'transformers', 'datasets', 'accelerate']

print("\nğŸ“¦ Packages hiá»‡n táº¡i:")
for pkg in key_packages:
    if pkg in installed_packages:
        version = pkg_resources.get_distribution(pkg).version
        print(f"   âœ… {pkg}: {version}")
    else:
        print(f"   âŒ {pkg}: chÆ°a cÃ i Ä‘áº·t")

print("\n" + "=" * 50)

# =============================================================================
# BÆ¯á»šC 2: Gá»  CÃ€I Äáº¶T PACKAGES CÅ¨ VÃ€ CÃ€I Äáº¶T Má»šI
# =============================================================================

print("ğŸ§¹ Dá»ŒNG Dáº¸P VÃ€ CÃ€I Äáº¶T PACKAGES Má»šI")
print("=" * 50)

# Gá»¡ cÃ i Ä‘áº·t packages cÅ© Ä‘á»ƒ trÃ¡nh xung Ä‘á»™t
print("ğŸ—‘ï¸ Gá»¡ cÃ i Ä‘áº·t packages cÅ©...")
!pip uninstall -y torch torchvision torchaudio transformers datasets accelerate peft -q

# CÃ i Ä‘áº·t PyTorch vá»›i CUDA 12.1 support
print("ğŸ”¥ CÃ i Ä‘áº·t PyTorch vá»›i CUDA support...")
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -q

# CÃ i Ä‘áº·t Transformers version má»›i nháº¥t há»— trá»£ Qwen3
print("ğŸ¤– CÃ i Ä‘áº·t Transformers >= 4.51.0 cho Qwen3...")
!pip install transformers>=4.51.0 -q

# CÃ i Ä‘áº·t cÃ¡c dependencies khÃ¡c
print("ğŸ“š CÃ i Ä‘áº·t datasets, accelerate, tokenizers...")
!pip install datasets>=2.21.0 accelerate>=0.34.0 tokenizers>=0.19.1 -q

# CÃ i Ä‘áº·t PEFT cho parameter-efficient fine-tuning
print("âš¡ CÃ i Ä‘áº·t PEFT...")
!pip install peft>=0.12.0 -q

# CÃ i Ä‘áº·t thÃªm cÃ¡c utilities
print("ğŸ› ï¸ CÃ i Ä‘áº·t utilities...")
!pip install packaging tqdm -q

print("\nâœ… CÃ€I Äáº¶T HOÃ€N Táº¤T!")
print("âš ï¸  QUAN TRá»ŒNG: RESTART RUNTIME NGAY BÃ‚Y GIá»œ!")
print("   ğŸ‘† VÃ o: Runtime > Restart session")
print("   ğŸ”„ Sau Ä‘Ã³ cháº¡y cell tiáº¿p theo")

# =============================================================================
# BÆ¯á»šC 3: XÃC MINH CÃ€I Äáº¶T (CHáº Y SAU KHI RESTART)
# =============================================================================

print("ğŸ” XÃC MINH CÃ€I Äáº¶T SAU RESTART")
print("=" * 50)

import torch
import transformers
import datasets
import accelerate
from packaging import version

# Hiá»ƒn thá»‹ thÃ´ng tin versions
print("ğŸ“‹ THÃ”NG TIN PACKAGES:")
print(f"   ğŸ”¥ PyTorch: {torch.__version__}")
print(f"   ğŸ¤– Transformers: {transformers.__version__}")
print(f"   ğŸ“Š Datasets: {datasets.__version__}")
print(f"   âš¡ Accelerate: {accelerate.__version__}")

# Kiá»ƒm tra CUDA
print(f"\nğŸ’» THÃ”NG TIN PHáº¦N Cá»¨NG:")
print(f"   ğŸ® CUDA available: {torch.cuda.is_available()}")

if torch.cuda.is_available():
    print(f"   ğŸ¯ GPU: {torch.cuda.get_device_name(0)}")
    print(f"   ğŸ”¢ CUDA version: {torch.version.cuda}")
    print(f"   ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"   ğŸ†” GPU Count: {torch.cuda.device_count()}")

# Kiá»ƒm tra version Transformers
required_version = "4.51.0"
current_version = transformers.__version__

print(f"\nğŸ” KIá»‚M TRA COMPATIBILITY:")
if version.parse(current_version) >= version.parse(required_version):
    print(f"   âœ… Transformers OK: {current_version} >= {required_version}")
else:
    print(f"   âŒ Transformers cÅ©: {current_version} < {required_version}")
    print("   ğŸ”§ Cháº¡y: !pip install transformers>=4.51.0 --upgrade")

# Test imports quan trá»ng
print(f"\nğŸ§ª TEST IMPORTS:")
try:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling
    from datasets import Dataset
    print("   âœ… Táº¥t cáº£ imports thÃ nh cÃ´ng!")
except ImportError as e:
    print(f"   âŒ Import error: {e}")

print("\nğŸ‰ Sáº´N SÃ€NG Báº®T Äáº¦U FINE-TUNING!")

# =============================================================================
# BÆ¯á»šC 4: Táº¢I MODEL QWEN3-0.6B
# =============================================================================

print("ğŸ¤– Táº¢I QWEN3-0.6B MODEL")
print("=" * 50)

from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling
)
from datasets import Dataset
import torch
import gc
import json

# Dá»n cache GPU
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    gc.collect()
    print("ğŸ§¹ ÄÃ£ dá»n cache GPU")

# Thiáº¿t láº­p device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ğŸ’» Sá»­ dá»¥ng device: {device}")

# Model configuration
model_name = "Qwen/Qwen3-0.6B"
print(f"ğŸ“¥ Äang táº£i {model_name}...")

try:
    # Táº£i tokenizer
    print("ğŸ”¤ Äang táº£i tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_name, 
        trust_remote_code=True,
        use_fast=True
    )
    print("   âœ… Tokenizer Ä‘Ã£ táº£i xong")
    
    # Táº£i model vá»›i cáº¥u hÃ¬nh tá»‘i Æ°u cho Colab
    print("ğŸ§  Äang táº£i model (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)...")
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
        device_map="auto" if torch.cuda.is_available() else None,
        low_cpu_mem_usage=True
    )
    print("   âœ… Model Ä‘Ã£ táº£i xong")
    
    # Cáº¥u hÃ¬nh tokenizer
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
        print("   âœ… ÄÃ£ cáº¥u hÃ¬nh pad token")
    
    # ThÃ´ng tin model
    print(f"\nğŸ“Š THÃ”NG TIN MODEL:")
    print(f"   ğŸ”¢ Sá»‘ parameters: {model.num_parameters():,}")
    print(f"   ğŸ’» Device: {next(model.parameters()).device}")
    print(f"   ğŸ¯ Data type: {next(model.parameters()).dtype}")
    print(f"   ğŸ—ï¸ Model type: {model.config.model_type}")
    
    # ThÃ´ng tin GPU memory
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(0) / 1024**3
        reserved = torch.cuda.memory_reserved(0) / 1024**3
        print(f"   ğŸ’¾ GPU memory allocated: {allocated:.2f} GB")
        print(f"   ğŸ’¾ GPU memory reserved: {reserved:.2f} GB")
    
    print("\nâœ… MODEL ÄÃƒ Sáº´N SÃ€NG CHO FINE-TUNING!")
    
except Exception as e:
    print(f"âŒ Lá»—i khi táº£i model: {e}")
    import traceback
    traceback.print_exc()

# =============================================================================
# BÆ¯á»šC 5: CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING
# =============================================================================

print("\nğŸ“š CHUáº¨N Bá»Š Dá»® LIá»†U TRAINING")
print("=" * 50)

# Dá»¯ liá»‡u training vá»›i ná»™i dung tiáº¿ng Viá»‡t Ä‘a dáº¡ng
training_data = [
    {
        "input": "Xin chÃ o, báº¡n cÃ³ thá»ƒ giá»›i thiá»‡u vá» báº£n thÃ¢n khÃ´ng?",
        "output": "ChÃ o báº¡n! TÃ´i lÃ  Qwen3, má»™t AI assistant thÃ´ng minh Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Alibaba Cloud. TÃ´i cÃ³ thá»ƒ giÃºp báº¡n tráº£ lá»i cÃ¢u há»i, viáº¿t code, phÃ¢n tÃ­ch dá»¯ liá»‡u, dá»‹ch thuáº­t vÃ  nhiá»u tÃ¡c vá»¥ khÃ¡c. TÃ´i Ä‘Æ°á»£c fine-tune Ä‘á»ƒ hiá»ƒu vÃ  giao tiáº¿p báº±ng tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn. Báº¡n cáº§n tÃ´i giÃºp gÃ¬ khÃ´ng?"
    },
    {
        "input": "Viáº¿t má»™t hÃ m Python tÃ­nh sá»‘ Fibonacci",
        "output": "ÄÃ¢y lÃ  cÃ¡c cÃ¡ch tÃ­nh sá»‘ Fibonacci trong Python:\n\n```python\n# PhÆ°Æ¡ng phÃ¡p Ä‘á»‡ quy Ä‘Æ¡n giáº£n\ndef fibonacci_recursive(n):\n    if n <= 1:\n        return n\n    return fibonacci_recursive(n-1) + fibonacci_recursive(n-2)\n\n# PhÆ°Æ¡ng phÃ¡p tá»‘i Æ°u vá»›i memoization\ndef fibonacci_memo(n, memo={}):\n    if n in memo:\n        return memo[n]\n    if n <= 1:\n        memo[n] = n\n    else:\n        memo[n] = fibonacci_memo(n-1, memo) + fibonacci_memo(n-2, memo)\n    return memo[n]\n\n# PhÆ°Æ¡ng phÃ¡p láº·p (hiá»‡u quáº£ nháº¥t)\ndef fibonacci_iterative(n):\n    if n <= 1:\n        return n\n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        a, b = b, a + b\n    return b\n\n# Test\nfor i in range(10):\n    print(f'F({i}) = {fibonacci_iterative(i)}')\n```"
    },
    {
        "input": "Machine Learning lÃ  gÃ¬? Giáº£i thÃ­ch cho ngÆ°á»i má»›i báº¯t Ä‘áº§u",
        "output": "Machine Learning (ML) lÃ  má»™t nhÃ¡nh cá»§a trÃ­ tuá»‡ nhÃ¢n táº¡o cho phÃ©p mÃ¡y tÃ­nh há»c tá»« dá»¯ liá»‡u:\n\n**VÃ­ dá»¥ Ä‘Æ¡n giáº£n:**\n- Thay vÃ¬ láº­p trÃ¬nh 'náº¿u email cÃ³ tá»« spam thÃ¬ lÃ  thÆ° rÃ¡c'\n- ML sáº½ há»c tá»« hÃ ng nghÃ¬n email Ä‘á»ƒ tá»± nháº­n diá»‡n spam\n\n**3 loáº¡i chÃ­nh:**\n1. **Supervised Learning**: Há»c tá»« dá»¯ liá»‡u cÃ³ nhÃ£n\n2. **Unsupervised Learning**: TÃ¬m pattern trong dá»¯ liá»‡u\n3. **Reinforcement Learning**: Há»c qua thá»­-sai vÃ  reward\n\n**á»¨ng dá»¥ng:** Gá»£i Ã½ sáº£n pháº©m, nháº­n dáº¡ng giá»ng nÃ³i, chatbot, dá»± Ä‘oÃ¡n giÃ¡ cá»• phiáº¿u...\n\nBáº¡n muá»‘n tÃ¬m hiá»ƒu sÃ¢u vá» pháº§n nÃ o?"
    },
    {
        "input": "CÃ¡ch há»c láº­p trÃ¬nh hiá»‡u quáº£ cho ngÆ°á»i má»›i?",
        "output": "Lá»™ trÃ¬nh há»c láº­p trÃ¬nh hiá»‡u quáº£:\n\n**BÆ°á»›c 1: Chá»n ngÃ´n ngá»¯ Ä‘áº§u tiÃªn (2-4 tuáº§n)**\n- Python: dá»… há»c, á»©ng dá»¥ng rá»™ng\n- JavaScript: cho web development\n- Java: cho enterprise apps\n\n**BÆ°á»›c 2: Náº¯m vá»¯ng ná»n táº£ng (4-8 tuáº§n)**\n- Biáº¿n, vÃ²ng láº·p, Ä‘iá»u kiá»‡n\n- Functions vÃ  data structures\n- Debugging vÃ  error handling\n\n**BÆ°á»›c 3: Thá»±c hÃ nh project (8-12 tuáº§n)**\n- Calculator, todo app\n- Web scraper, mini game\n- Database interaction\n\n**Tips thÃ nh cÃ´ng:**\n- Code má»—i ngÃ y Ã­t nháº¥t 1 giá»\n- Tham gia community (Stack Overflow)\n- Äá»c code ngÆ°á»i khÃ¡c trÃªn GitHub\n- Äá»«ng chá»‰ xem video, hÃ£y code theo\n\nKiÃªn trÃ¬ lÃ  chÃ¬a khÃ³a!"
    },
    {
        "input": "Giáº£i thÃ­ch thuáº­t toÃ¡n Quick Sort",
        "output": "Quick Sort sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p 'divide and conquer':\n\n**NguyÃªn lÃ½:**\n1. Chá»n má»™t pháº§n tá»­ lÃ m pivot\n2. Partition: Ä‘Æ°a pháº§n tá»­ < pivot vá» trÃ¡i, > pivot vá» pháº£i\n3. Recursively sort cÃ¡c pháº§n con\n\n**Code Python:**\n```python\ndef quicksort(arr):\n    if len(arr) <= 1:\n        return arr\n    \n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    \n    return quicksort(left) + middle + quicksort(right)\n\n# Test\narr = [3, 6, 8, 10, 1, 2, 1]\nsorted_arr = quicksort(arr)\nprint(sorted_arr)  # [1, 1, 2, 3, 6, 8, 10]\n```\n\n**Äá»™ phá»©c táº¡p:**\n- Best/Average: O(n log n)\n- Worst case: O(nÂ²)\n- Space: O(log n)"
    },
    {
        "input": "REST API lÃ  gÃ¬? CÃ¡ch thiáº¿t káº¿ API tá»‘t?",
        "output": "REST API lÃ  kiáº¿n trÃºc thiáº¿t káº¿ web services:\n\n**NguyÃªn táº¯c REST:**\n- Stateless: má»—i request Ä‘á»™c láº­p\n- Resource-based: URL Ä‘áº¡i diá»‡n cho resources\n- HTTP methods: GET, POST, PUT, DELETE\n- JSON format cho data\n\n**VÃ­ dá»¥ thiáº¿t káº¿:**\n```\nGET /api/users          # Láº¥y danh sÃ¡ch users\nGET /api/users/123      # Láº¥y user cÃ³ id=123\nPOST /api/users         # Táº¡o user má»›i\nPUT /api/users/123      # Update user 123\nDELETE /api/users/123   # XÃ³a user 123\n```\n\n**Best practices:**\n- Sá»­ dá»¥ng nouns trong URL\n- Proper HTTP status codes\n- Versioning: /api/v1/users\n- Authentication & authorization\n- Rate limiting\n- Documentation (Swagger)"
    },
    {
        "input": "Docker lÃ  gÃ¬ vÃ  táº¡i sao nÃªn sá»­ dá»¥ng?",
        "output": "Docker lÃ  platform containerization:\n\n**Container vs VM:**\n- Container: Chia sáº» OS kernel, nháº¹ hÆ¡n\n- VM: Má»—i VM cÃ³ OS riÃªng, náº·ng hÆ¡n\n\n**Æ¯u Ä‘iá»ƒm Docker:**\n- Lightweight vÃ  fast startup\n- Portable: 'Build once, run anywhere'\n- Isolated: á»¨ng dá»¥ng cÃ´ láº­p\n- Scalable: Dá»… scale up/down\n\n**Basic commands:**\n```bash\ndocker pull nginx\ndocker run -p 8080:80 nginx\ndocker ps\ndocker stop container_id\n```\n\n**Dockerfile example:**\n```dockerfile\nFROM node:18-alpine\nWORKDIR /app\nCOPY package*.json ./\nRUN npm install\nCOPY . .\nEXPOSE 3000\nCMD ['npm', 'start']\n```"
    },
    {
        "input": "Blockchain hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o?",
        "output": "Blockchain lÃ  sá»• cÃ¡i phÃ¢n tÃ¡n ghi láº¡i giao dá»‹ch:\n\n**Cáº¥u trÃºc:**\n- Chuá»—i cÃ¡c block liÃªn káº¿t báº±ng hash\n- Má»—i block chá»©a: hash, previous hash, transactions, timestamp\n- Immutable: KhÃ´ng thá»ƒ thay Ä‘á»•i sau khi confirm\n\n**Äáº·c Ä‘iá»ƒm:**\n- Decentralized: KhÃ´ng cÃ³ cÆ¡ quan trung Æ°Æ¡ng\n- Transparent: Má»i giao dá»‹ch cÃ´ng khai\n- Secure: MÃ£ hÃ³a cryptographic\n\n**Consensus mechanisms:**\n- Proof of Work (Bitcoin): Miners giáº£i puzzle\n- Proof of Stake (Ethereum 2.0): Validators stake token\n\n**á»¨ng dá»¥ng:**\n- Cryptocurrency (Bitcoin, Ethereum)\n- Smart contracts\n- Supply chain tracking\n- Digital identity\n- Voting systems"
    }
]

print(f"ğŸ“Š ÄÃ£ táº¡o {len(training_data)} vÃ­ dá»¥ training")

# =============================================================================
# BÆ¯á»šC 6: FORMAT Dá»® LIá»†U CHO QWEN3
# =============================================================================

def format_qwen3_chat(input_text, output_text):
    """Format dá»¯ liá»‡u theo Qwen3 chat format"""
    return f"<|im_start|>user\n{input_text}<|im_end|>\n<|im_start|>assistant\n{output_text}<|im_end|>"

# Format training data
print("ğŸ”„ Äang format dá»¯ liá»‡u cho Qwen3...")
formatted_texts = []
for item in training_data:
    text = format_qwen3_chat(item['input'], item['output'])
    formatted_texts.append(text)

print("ğŸ“ VÃ­ dá»¥ formatted text:")
print(formatted_texts[0][:200] + "...")

# Function tokenize
def tokenize_function(examples):
    return tokenizer(
        examples["text"],
        truncation=True,
        padding="max_length",
        max_length=1024,  # Sá»­ dá»¥ng 1K tokens cho training nhanh hÆ¡n
        return_tensors="pt"
    )

# Táº¡o dataset
try:
    print("ğŸ“Š Äang táº¡o dataset...")
    dataset = Dataset.from_dict({"text": formatted_texts})
    tokenized_dataset = dataset.map(
        tokenize_function, 
        batched=True, 
        remove_columns=["text"],
        desc="Tokenizing dataset"
    )
    print(f"âœ… Dataset Ä‘Ã£ táº¡o: {len(tokenized_dataset)} examples")
    print(f"ğŸ“ Sample token length: {len(tokenized_dataset[0]['input_ids'])}")
    
except Exception as e:
    print(f"âŒ Lá»—i táº¡o dataset: {e}")

# =============================================================================
# BÆ¯á»šC 7: THIáº¾T Láº¬P TRAINING
# =============================================================================

print("\nâš™ï¸ THIáº¾T Láº¬P TRAINING CONFIGURATION")
print("=" * 50)

# Data collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False,  # KhÃ´ng sá»­ dá»¥ng masked language modeling
)

# Training arguments Ä‘Æ°á»£c tá»‘i Æ°u cho Colab
training_args = TrainingArguments(
    output_dir="./qwen3_colab_results",
    overwrite_output_dir=True,
    
    # Training parameters
    num_train_epochs=3,
    per_device_train_batch_size=2,  # Batch size nhá» cho Colab
    gradient_accumulation_steps=4,   # TÄƒng effective batch size
    
    # Learning rate
    learning_rate=1e-5,              # Learning rate tháº¥p cho Qwen3
    warmup_steps=100,
    lr_scheduler_type="cosine",
    
    # Optimization
    weight_decay=0.01,
    max_grad_norm=1.0,
    optim="adamw_torch",
    
    # Memory optimization
    gradient_checkpointing=True,     # Tiáº¿t kiá»‡m memory
    fp16=torch.cuda.is_available(),  # Mixed precision training
    bf16=False,
    dataloader_num_workers=0,        # TrÃ¡nh lá»—i multiprocessing trÃªn Colab
    
    # Logging vÃ  saving
    logging_steps=5,
    save_steps=100,
    save_total_limit=2,
    prediction_loss_only=True,
    remove_unused_columns=False,
    
    # Misc
    report_to=None,                  # Táº¯t wandb/tensorboard
    load_best_model_at_end=False,
    ddp_find_unused_parameters=False,
)

# Táº¡o trainer
try:
    print("ğŸ—ï¸ Äang táº¡o Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer,
    )
    print("âœ… Trainer Ä‘Ã£ sáºµn sÃ ng!")
    
    # Hiá»ƒn thá»‹ cáº¥u hÃ¬nh training
    print(f"\nğŸ“‹ TRAINING CONFIGURATION:")
    print(f"   ğŸ¯ Model: {model_name}")
    print(f"   ğŸ’» Device: {device}")
    print(f"   ğŸ“Š Training examples: {len(tokenized_dataset)}")
    print(f"   ğŸ”„ Epochs: {training_args.num_train_epochs}")
    print(f"   ğŸ“¦ Batch size: {training_args.per_device_train_batch_size}")
    print(f"   âš¡ Gradient accumulation: {training_args.gradient_accumulation_steps}")
    print(f"   ğŸ“ˆ Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}")
    print(f"   ğŸ“š Learning rate: {training_args.learning_rate}")
    print(f"   ğŸ¯ Mixed precision: {training_args.fp16}")
    
except Exception as e:
    print(f"âŒ Lá»—i táº¡o trainer: {e}")

# =============================================================================
# BÆ¯á»šC 8: Báº®T Äáº¦U TRAINING
# =============================================================================

print("\n" + "=" * 60)
print("ğŸš€ Báº®T Äáº¦U QWEN3-0.6B FINE-TUNING TRÃŠN COLAB")
print("=" * 60)

try:
    # Dá»n cache trÆ°á»›c khi training
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()
    
    print("â° Training báº¯t Ä‘áº§u... (cÃ³ thá»ƒ máº¥t 10-30 phÃºt)")
    print("ğŸ“Š Theo dÃµi loss Ä‘á»ƒ xem progress")
    print("-" * 40)
    
    # Báº¯t Ä‘áº§u training
    training_output = trainer.train()
    
    print("\n" + "=" * 40)
    print("ğŸ‰ TRAINING HOÃ€N THÃ€NH THÃ€NH CÃ”NG!")
    print("=" * 40)
    print(f"ğŸ“Š Final training loss: {training_output.training_loss:.4f}")
    
except Exception as e:
    print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh training: {e}")
    import traceback
    traceback.print_exc()

# =============================================================================
# BÆ¯á»šC 9: LÆ¯U MODEL
# =============================================================================

print("\nğŸ’¾ LÆ¯U MODEL ÄÃƒ FINE-TUNE")
print("=" * 50)

output_dir = "./qwen3_0.6b_finetuned"

try:
    print(f"ğŸ“ Äang lÆ°u model vÃ o {output_dir}...")
    trainer.save_model(output_dir)
    tokenizer.save_pretrained(output_dir)
    print("âœ… Model Ä‘Ã£ Ä‘Æ°á»£c lÆ°u thÃ nh cÃ´ng!")
    
    # LÆ°u vÃ o Google Drive (optional)
    print("\nâ˜ï¸ LÆ°u vÃ o Google Drive...")
    try:
        from google.colab import drive
        drive.mount('/content/drive', force_remount=True)
        
        import shutil
        drive_path = '/content/drive/MyDrive/qwen3_0.6b_finetuned'
        
        # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
        import os
        os.makedirs(os.path.dirname(drive_path), exist_ok=True)
        
        # Copy model
        if os.path.exists(drive_path):
            shutil.rmtree(drive_path)
        shutil.copytree(output_dir, drive_path)
        
        print(f"âœ… Model cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o Google Drive: {drive_path}")
        
    except Exception as drive_error:
        print(f"âš ï¸ KhÃ´ng thá»ƒ lÆ°u vÃ o Google Drive: {drive_error}")
        print("ğŸ’¡ Báº¡n cÃ³ thá»ƒ download model tá»« thÆ° má»¥c ./qwen3_0.6b_finetuned")
    
except Exception as e:
    print(f"âŒ Lá»—i khi lÆ°u model: {e}")

# =============================================================================
# BÆ¯á»šC 10: TEST MODEL ÄÃƒ FINE-TUNE
# =============================================================================

print("\nğŸ§ª TEST QWEN3 MODEL ÄÃƒ FINE-TUNE")
print("=" * 50)

def test_qwen3(prompt, max_new_tokens=200, temperature=0.7):
    """Test Qwen3 vá»›i prompt"""
    # Format prompt theo Qwen3 chat format
    full_prompt = f"<|im_start|>user\n{prompt}<|im_end|>\n<|im_start|>assistant\n"
    
    # Tokenize input
    inputs = tokenizer(full_prompt, return_tensors="pt")
    if torch.cuda.is_available():
        inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # Generate response
    model.eval()
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=0.8,
            top_k=20,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1
        )
    
    # Decode response
    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)
    response = full_response[len(full_prompt):].split("<|im_end|>")[0].strip()
    return response

# Test cases
test_prompts = [
    "Xin chÃ o, báº¡n khá»e khÃ´ng?",
    "Viáº¿t code Python Ä‘á»c file CSV",
    "Giáº£i thÃ­ch vá» Neural Networks",
    "CÃ¡ch tá»‘i Æ°u database query?",
    "So sÃ¡nh Python vÃ  JavaScript",
    "TÃ´i má»›i há»c láº­p trÃ¬nh, báº¯t Ä‘áº§u tá»« Ä‘Ã¢u?"
]

print("ğŸ” Äang test model vá»›i cÃ¡c cÃ¢u há»i máº«u...\n")

for i, prompt in enumerate(test_prompts, 1):
    print(f"ğŸ” Test {i}/6:")
    print(f"ğŸ‘¤ Input: {prompt}")
    print(f"ğŸ¤– Qwen3 Output:")
    
    try:
        response = test_qwen3(prompt)
        print(response)
        print("-" * 50)
        
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        print("-" * 50)

print("ğŸ HOÃ€N THÃ€NH Táº¤T Cáº¢ TESTS!")

# =============================================================================
# BÆ¯á»šC 11: HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG MODEL
# =============================================================================

print("\nğŸ“– HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG MODEL ÄÃƒ FINE-TUNE")
print("=" * 50)

usage_guide = """
ğŸ‰ CHÃšC Má»ªNG! Báº¡n Ä‘Ã£ fine-tune thÃ nh cÃ´ng Qwen3-0.6B!

ğŸ“ MODEL LOCATION:
   - Local: ./qwen3_0.6b_finetuned/
   - Google Drive: /content/drive/MyDrive/qwen3_0.6b_finetuned/

ğŸ’¡ CÃCH Sá»¬ Dá»¤NG MODEL:

1. Load model Ä‘Ã£ fine-tune: